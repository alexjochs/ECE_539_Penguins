{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexjochs/ECE_539_Penguins/blob/preprocess/PenguinsPreprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "zjyEyhfEEczD",
        "outputId": "c465d0d6-2263-48ee-ca71-ed3aad65b7ca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b9c0455014ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# gdrive_data_filepath = r\"/content/gdrive/MyDrive/'Penguin_counting'/data_peng_watch\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Oscar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mgdrive_data_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'/content/drive/MyDrive/Colab\\ Notebooks/539\\ Project/data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    107\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 128\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Alex\n",
        "# drive.mount('/content/gdrive')\n",
        "# gdrive_data_filepath = r\"/content/gdrive/MyDrive/'Penguin_counting'/data_peng_watch\"\n",
        "# Oscar\n",
        "drive.mount('/content/drive')\n",
        "gdrive_data_filepath = r'/content/drive/MyDrive/Colab\\ Notebooks/539\\ Project/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsV513awVrjd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from matplotlib import image\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.ndimage.filters import gaussian_filter \n",
        "import scipy\n",
        "from scipy.spatial import KDTree\n",
        "import h5py\n",
        "import time\n",
        "from PIL import Image\n",
        "import glob\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-WUgAd0WF7v"
      },
      "outputs": [],
      "source": [
        "MASTER_LIST = ['BAILa', 'DAMOa', 'HALFb', 'HALFc', 'LOCKb', 'MAIVb', 'MAIVc', 'NEKOa', 'NEKOb', 'NEKOc', 'PETEc', 'PETEd', 'PETEe', 'PETEf', 'SPIGa', 'GEORa']\n",
        "VM_ROOT = r'/content'\n",
        "cwd = None\n",
        "def get_new_batch(target=None):\n",
        "    assert target is not None, f\"can't get specific folder: {target} and load all data\"\n",
        "    tgz_name = target + '.tgz'\n",
        "    folder_path = os.path.join(gdrive_data_filepath, tgz_name)\n",
        "    !tar --gunzip --extract --file={folder_path} --directory {VM_ROOT}\n",
        "    cwd = os.path.join(VM_ROOT, target)\n",
        "\n",
        "def save_batch_to_drive(target=None):\n",
        "    # take working files, save them back to Gdrive\n",
        "    assert target is not None, f\"can't get specific folder: {target} and load all data\"\n",
        "    tgz_name = target + '.tgz'\n",
        "    folder_path = os.path.join(VM_ROOT, target)\n",
        "    # Oscar\n",
        "    os.chdir('/content/drive/MyDrive/Colab Notebooks/539 Project/data') \n",
        "    # Alex\n",
        "    # os.chdir(gdrive_data_filepath)\n",
        "    # !tar -czvf {tgz_name} -P {folder_path} # Verbose\n",
        "    !tar -czf {tgz_name} -P {folder_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW-k_xXHWiZc"
      },
      "outputs": [],
      "source": [
        "annotations_path = VM_ROOT + r'/CompleteAnnotations_2016-07-11'\n",
        "\n",
        "def run_all():\n",
        "    json_filepath_list = get_json_files_from_folder()\n",
        "    for filepath in json_filepath_list:\n",
        "        df = run(filepath_=filepath)\n",
        "        save_df_as_json(df)\n",
        "\n",
        "def run(filepath_=None, target=None):\n",
        "    if target is not None:\n",
        "        filepath_ = annotations_path + r'/' + target + '.json'\n",
        "    data_group_name = filepath_[-10:-5]\n",
        "    df = load_json_as_df(filepath_)\n",
        "    df.loc[df.xy.isnull(), 'xy'] = [[]]\n",
        "    # have to check if inner list has na values as well :/\n",
        "    df_xy_ = df['xy']\n",
        "    for index, value in df_xy_.items():\n",
        "        if len(value) > 0:\n",
        "            while '_NaN_' in value:\n",
        "                value.remove('_NaN_')\n",
        "                df_xy_.at[index] = value\n",
        "            while None in value:\n",
        "                value.remove(None)\n",
        "                df_xy_.at[index] = value\n",
        "            if len(value) == 0:\n",
        "                df_xy_.at[index] = [[]]\n",
        "        else:\n",
        "            df_xy_.at[index] = [[]]\n",
        "    # df_xy_ = to_1D(df['xy'])\n",
        "    # if df_xy_.isna().sum() > 0:\n",
        "    #     df_xy_na_mask = df_xy_.isna()\n",
        "    #     df.loc[df_xy_na_mask, 'xy'] = [[]]\n",
        "    df['xy'] = df_xy_\n",
        "    return df\n",
        "\n",
        "def save_df_as_json(df):\n",
        "    data_group_filename = df['imName'].iloc[0][:5] + '.json'\n",
        "    try:\n",
        "        os.mkdir('/content/annotations')\n",
        "    except FileExistsError as e:\n",
        "        print('looks like local annotations folder already exists!')\n",
        "    print(data_group_filename)\n",
        "    with open(os.path.join('/content/annotations', data_group_filename), 'w') as json_file:\n",
        "        json.dump(json.loads(df.to_json(orient='records')), json_file)\n",
        "\n",
        "def get_json_files_from_folder():\n",
        "    json_filepath_list = []\n",
        "    for filename in os.listdir(annotations_path):\n",
        "        f = os.path.join(annotations_path, filename)\n",
        "        if os.path.isfile(f):\n",
        "            file_extension = os.path.splitext(f)[1]\n",
        "        if file_extension == '.json':\n",
        "            json_filepath_list.append(f)\n",
        "    return json_filepath_list\n",
        "\n",
        "def load_json_as_df(filepath):\n",
        "    with open(filepath,'r') as json_file:\n",
        "        json_data = json.loads(json_file.read())\n",
        "    return pd.json_normalize(json_data, record_path =['dots'])\n",
        "\n",
        "def to_1D(series):\n",
        "    return pd.Series([x for _list in series for x in _list])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQuIbI08WsIb"
      },
      "outputs": [],
      "source": [
        "# dots: 2d list of xy dots from users\n",
        "# return longest (ie most penguins clicked) 1d list\n",
        "def get_longest_dot_list(dots):\n",
        "    if len(dots) == 0:\n",
        "        return []\n",
        "    idx = 0\n",
        "    if len(dots) > 1:\n",
        "        for i, x in enumerate(dots):\n",
        "            if type(x[0]) != type(0):\n",
        "                if len(x) >= len(dots[idx]):\n",
        "                    idx = i\n",
        "    return dots[idx]\n",
        "\n",
        "# One hot encoding of user clicks\n",
        "def make_sparse_mat(img_shape, dots):\n",
        "    mat = np.zeros((img_shape[1], img_shape[0]))\n",
        "\n",
        "    # Check for empty dots list case TODO\n",
        "    if len(dots) == 0:\n",
        "        return mat\n",
        "\n",
        "    # Check for 1-D case TODO\n",
        "    if type(dots[0]) == type(0):\n",
        "        mat[dots[1], dots[0]] = 1\n",
        "    else:\n",
        "        for dot in dots:\n",
        "            if not (dot[0] > 800 or dot[1] > 600 or dot[0] < 0 or dot[1] < 0):\n",
        "                mat[dot[1], dot[0]] = 1\n",
        "\n",
        "    return mat\n",
        "\n",
        "def downsample_dots(dots, img_shape):\n",
        "    \"\"\"convert xy coords of annotations down to 600x800 img space\"\"\"\n",
        "    ds_dots = []\n",
        "    x_scaler = 800.0 / float(img_shape[0])\n",
        "    y_scaler = 600.0 / float(img_shape[1])\n",
        "\n",
        "    # Check for empty dots list case\n",
        "    if len(dots) == 0:\n",
        "        return []\n",
        "\n",
        "    # Check for 1-D case TODO\n",
        "    if type(dots[0]) == type(0):\n",
        "        if dots[0] >= img_shape[0] or dots[1] >= img_shape[1] or dots[0] < 0 or dots[1] < 0:\n",
        "            print('Dropped 1 dot - (100%)') # TODO\n",
        "            return []\n",
        "        return [math.floor(dots[0] * x_scaler), math.floor(dots[1] * y_scaler)]\n",
        "\n",
        "    for dot in dots:\n",
        "        try:\n",
        "            if dot[0] >= img_shape[0] or dot[1] >= img_shape[1] or dot[0] < 0 or dot[1] < 0: # if == could we just subtract one? i.e. [2048, 1536] - > [2047, 1535]\n",
        "                # print('DOT OUT OF RANGE:', dot)\n",
        "                continue\n",
        "            ds_dots.append([math.floor(dot[0] * x_scaler), math.floor(dot[1] * y_scaler)]) # TODO\n",
        "        except:\n",
        "            print('ERROR -', dot) # TODO\n",
        "\n",
        "    if len(ds_dots) != len(dots):    \n",
        "        print('Dropped', str(len(dots)-len(ds_dots)), 'dots out of', str(len(dots)), '-', '(' + str((len(dots)-len(ds_dots))*100/len(dots)) + '%)')\n",
        "    return ds_dots\n",
        "\n",
        "def gaussian_filter_density(gt):\n",
        "    #Generates a density map using Gaussian filter transformation\n",
        "    \n",
        "    density = np.zeros(gt.shape, dtype=np.float32)\n",
        "    \n",
        "    gt_count = np.count_nonzero(gt)\n",
        "    \n",
        "    if gt_count == 0:\n",
        "        return density\n",
        "\n",
        "    # FInd out the K nearest neighbours using a KDTree\n",
        "    \n",
        "    pts = np.array(list(zip(np.nonzero(gt)[1].ravel(), np.nonzero(gt)[0].ravel())))\n",
        "    leafsize = 2048\n",
        "    \n",
        "    # build kdtree\n",
        "    tree = scipy.spatial.KDTree(pts.copy(), leafsize=leafsize)\n",
        "    \n",
        "    # query kdtree\n",
        "    distances, locations = tree.query(pts, k=4)\n",
        "\n",
        "        \n",
        "    for i, pt in enumerate(pts):\n",
        "        pt2d = np.zeros(gt.shape, dtype=np.float32)\n",
        "        pt2d[pt[1],pt[0]] = 1.\n",
        "        if gt_count > 1:\n",
        "            sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1\n",
        "        else:\n",
        "            sigma = np.average(np.array(gt.shape))/2./2. #case: 1 point\n",
        "        \n",
        "        #Convolve with the gaussian filter\n",
        "        \n",
        "        # density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode='constant')\n",
        "        input_ = np.fft.fft2(pt2d)\n",
        "        result = scipy.ndimage.fourier_gaussian(input_, sigma)\n",
        "        density = np.add(density, np.fft.ifft2(result).real, casting=\"unsafe\")\n",
        "    \n",
        "    return density"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7JSbZh4Cv0j"
      },
      "source": [
        "some ideas to improve speed:  \n",
        "- load all images into memory\n",
        "- downsample images to input size\n",
        "- - w/ fft, down to 4 sec!\n",
        "- use mutiprocessing to speed up execution\n",
        "- use fft instead of gaussian filter?\n",
        "- - It works! down to 35 sec per image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS7t8qWWuTnA"
      },
      "source": [
        "Cell below loads image sizes into original image data dict, which is something like {'BAILa': (2048, 1536), 'DAMOa': (2048, 1536), 'HALFb': (2048, 1536), 'HALFc': (1920, 1080), etc..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x40SzGTVoSJK"
      },
      "outputs": [],
      "source": [
        "# TODO I think we still want this\n",
        "original_img_dim = {'BAILa': (2048, 1536), 'DAMOa': (2048, 1536), 'HALFb': (2048, 1536), 'HALFc': (1920, 1080), 'LOCKb': (1920, 1080), 'MAIVb': (2048, 1536), 'MAIVc': (2048, 1536), 'NEKOa': (1920, 1080), 'NEKOb': (2048, 1536), 'NEKOc': (2048, 1536), 'PETEc': (2048, 1536), 'PETEd': (2048, 1536), 'PETEf': (2048, 1536), 'SPIGa': (1920, 1080), 'GEORa': (2048, 1536), 'PETEe': (2048, 1536)}\n",
        "# for data_split_name in working_list:\n",
        "#     local_directory = os.path.join(VM_ROOT, data_split_name)\n",
        "#     for filename in glob.glob(local_directory + '/*.' + 'JPG'): #assuming gif\n",
        "#         im=Image.open(filename)\n",
        "#         original_img_dim[data_split_name] = (im.size[0], im.size[1])\n",
        "#         break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dY263MRlPt4"
      },
      "source": [
        "---\n",
        "The greatest of for loops:  \n",
        "- for every datasplit in the master list:\n",
        "    - get images for datasplit, load in to ram\n",
        "    - get annotations, store in a df\n",
        "    - get image size for that datasplit (each split is diff)\n",
        "    - for **every image in datasplit**:  \n",
        "        - downsample image to (600,800)\n",
        "        - downsample annotations\n",
        "        - get groundtruth of image via fft\n",
        "        - save downsampled image and downsampled groundtruth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZVYBHuyhTNt"
      },
      "outputs": [],
      "source": [
        "# Get Annotations\n",
        "get_new_batch('CompleteAnnotations_2016-07-11')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crHDXBojrBBP"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/*_gt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJCyvwIQuP8Z",
        "outputId": "31f18409-b95a-4f3f-fba4-bb89f6188c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BAILa processing compelete.\n",
            "DAMOa processing compelete.\n",
            "HALFb processing compelete.\n",
            "HALFc processing compelete.\n",
            "LOCKb processing compelete.\n",
            "MAIVb processing compelete.\n",
            "MAIVc processing compelete.\n",
            "NEKOa processing compelete.\n",
            "NEKOb processing compelete.\n",
            "NEKOc processing compelete.\n",
            "PETEc processing compelete.\n",
            "PETEd processing compelete.\n",
            "PETEe processing compelete.\n",
            "PETEf processing compelete.\n",
            "SPIGa processing compelete.\n",
            "GEORa processing compelete.\n"
          ]
        }
      ],
      "source": [
        "# slice working list to split workload between us\n",
        "working_list = MASTER_LIST[:] # TODO\n",
        "\n",
        "# MASTER LOOP\n",
        "# os.chdir(VM_ROOT) TODO\n",
        "for data_split_name in working_list:\n",
        "    \n",
        "    if not os.path.exists(os.path.join('/content/drive/MyDrive/Colab Notebooks/539 Project/data', data_split_name + '_gt.tgz')):\n",
        "        print('\\nProcessing', data_split_name,'\\n----------------------')  \n",
        "\n",
        "        # Get images of batch TODO Necessary?\n",
        "        if not os.path.exists(os.path.join(VM_ROOT, data_split_name)):\n",
        "            get_new_batch(data_split_name)\n",
        "    \n",
        "        # Create split dir of ground truths\n",
        "        split_dir = os.path.join(VM_ROOT, data_split_name + '_gt')\n",
        "        os.mkdir(split_dir)\n",
        "        os.chdir(split_dir)\n",
        "\n",
        "        # Get annotations as DataFrame\n",
        "        df = run(target=data_split_name)\n",
        "\n",
        "        # Create heatmap for each image\n",
        "        for i, row in df.iterrows():\n",
        "            # Get image for size to pass to downsample\n",
        "            # img = Image.open(os.path.join('/content', data_split_name, row['imName']) + '.JPG')\n",
        "            # TODO: actually reshape image?\n",
        "            # downsample dots to new img size of 600,800\n",
        "            img_dots = row['xy']\n",
        "            dots = get_longest_dot_list(img_dots)\n",
        "            ds_dots = downsample_dots(dots, original_img_dim[data_split_name])\n",
        "\n",
        "            # make gt heatmap\n",
        "            k_ds = make_sparse_mat((800, 600), ds_dots)\n",
        "            k_ds = gaussian_filter_density(k_ds)\n",
        "\n",
        "            # save file as an h5 type\n",
        "            with h5py.File(row['imName'] + '_gt.h5', 'w') as f:\n",
        "                f['density'] = k_ds\n",
        "\n",
        "        # Save ground truth zip\n",
        "        save_batch_to_drive(target=data_split_name + '_gt')\n",
        "        print('Saved', data_split_name+'_gt.tgz')\n",
        "        !rm -rf /content/{data_split_name}\n",
        "        !rm -rf /content/{data_split_name}_gt\n",
        "        print('Removed', data_split_name, 'and', data_split_name+'_gt folders.')\n",
        "    print(data_split_name, 'processing compelete.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNIWE1Eq4tAm"
      },
      "outputs": [],
      "source": [
        "# # open each image\n",
        "# img = Image.open(os.path.join('/content', data_split_name, row['imName']) + '.JPG')\n",
        "# img.size\n",
        "# img = img.resize((800, 600), Image.ANTIALIAS)\n",
        "#         # image_list.append(img) TODO"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PenguinsPreprocess.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}