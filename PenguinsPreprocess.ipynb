{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexjochs/ECE_539_Penguins/blob/model/PenguinsPreprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjyEyhfEEczD",
        "outputId": "16a18a22-1c5d-4619-dae4-175619bd4b7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Alex\n",
        "drive.mount('/content/gdrive')\n",
        "gdrive_data_filepath = r\"/content/gdrive/MyDrive/Penguin_counting/data_peng_watch\"\n",
        "# Oscar\n",
        "# drive.mount('/content/drive')\n",
        "# gdrive_data_filepath = r'/content/drive/MyDrive/Colab\\ Notebooks/539\\ Project/data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsV513awVrjd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from matplotlib import image\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy.ndimage.filters import gaussian_filter \n",
        "import scipy\n",
        "from scipy.spatial import KDTree\n",
        "import h5py\n",
        "import time\n",
        "from PIL import Image\n",
        "import glob\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-WUgAd0WF7v"
      },
      "outputs": [],
      "source": [
        "MASTER_LIST = ['BAILa', 'DAMOa', 'HALFb', 'HALFc', 'LOCKb', 'MAIVb', 'MAIVc', 'NEKOa', 'NEKOb', 'NEKOc', 'PETEc', 'PETEd', 'PETEe', 'PETEf', 'SPIGa', 'GEORa']\n",
        "VM_ROOT = r'/content'\n",
        "cwd = None\n",
        "def get_new_batch(target=None):\n",
        "    assert target is not None, f\"can't get specific folder: {target} and load all data\"\n",
        "    tgz_name = target + '.tgz'\n",
        "    folder_path = os.path.join(gdrive_data_filepath, tgz_name)\n",
        "    !tar --gunzip --extract --file={folder_path} --directory {VM_ROOT}\n",
        "    cwd = os.path.join(VM_ROOT, target)\n",
        "\n",
        "def save_batch_to_drive(target=None):\n",
        "    # take working files, save them back to Gdrive\n",
        "    assert target is not None, f\"can't get specific folder: {target} and load all data\"\n",
        "    tgz_name = target + '.tgz'\n",
        "    folder_path = os.path.join(VM_ROOT, target)\n",
        "    # Oscar\n",
        "    # os.chdir('/content/drive/MyDrive/Colab Notebooks/539 Project/data') \n",
        "    # Alex\n",
        "    os.chdir(gdrive_data_filepath)\n",
        "    # !tar -czvf {tgz_name} -P {folder_path} # Verbose\n",
        "    !tar -czf {tgz_name} -P {folder_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW-k_xXHWiZc"
      },
      "outputs": [],
      "source": [
        "annotations_path = VM_ROOT + r'/CompleteAnnotations_2016-07-11'\n",
        "\n",
        "def run_all():\n",
        "    json_filepath_list = get_json_files_from_folder()\n",
        "    for filepath in json_filepath_list:\n",
        "        df = run(filepath_=filepath)\n",
        "        save_df_as_json(df)\n",
        "\n",
        "def run(filepath_=None, target=None):\n",
        "    if target is not None:\n",
        "        filepath_ = annotations_path + r'/' + target + '.json'\n",
        "    data_group_name = filepath_[-10:-5]\n",
        "    df = load_json_as_df(filepath_)\n",
        "    df.loc[df.xy.isnull(), 'xy'] = [[]]\n",
        "    # have to check if inner list has na values as well :/\n",
        "    df_xy_ = df['xy']\n",
        "    for index, value in df_xy_.items():\n",
        "        if len(value) > 0:\n",
        "            while '_NaN_' in value:\n",
        "                value.remove('_NaN_')\n",
        "                df_xy_.at[index] = value\n",
        "            while None in value:\n",
        "                value.remove(None)\n",
        "                df_xy_.at[index] = value\n",
        "            if len(value) == 0:\n",
        "                df_xy_.at[index] = [[]]\n",
        "        else:\n",
        "            df_xy_.at[index] = [[]]\n",
        "    # df_xy_ = to_1D(df['xy'])\n",
        "    # if df_xy_.isna().sum() > 0:\n",
        "    #     df_xy_na_mask = df_xy_.isna()\n",
        "    #     df.loc[df_xy_na_mask, 'xy'] = [[]]\n",
        "    df['xy'] = df_xy_\n",
        "    return df\n",
        "\n",
        "def save_df_as_json(df):\n",
        "    data_group_filename = df['imName'].iloc[0][:5] + '.json'\n",
        "    try:\n",
        "        os.mkdir('/content/annotations')\n",
        "    except FileExistsError as e:\n",
        "        print('looks like local annotations folder already exists!')\n",
        "    print(data_group_filename)\n",
        "    with open(os.path.join('/content/annotations', data_group_filename), 'w') as json_file:\n",
        "        json.dump(json.loads(df.to_json(orient='records')), json_file)\n",
        "\n",
        "def get_json_files_from_folder():\n",
        "    json_filepath_list = []\n",
        "    for filename in os.listdir(annotations_path):\n",
        "        f = os.path.join(annotations_path, filename)\n",
        "        if os.path.isfile(f):\n",
        "            file_extension = os.path.splitext(f)[1]\n",
        "        if file_extension == '.json':\n",
        "            json_filepath_list.append(f)\n",
        "    return json_filepath_list\n",
        "\n",
        "def load_json_as_df(filepath):\n",
        "    with open(filepath,'r') as json_file:\n",
        "        json_data = json.loads(json_file.read())\n",
        "    return pd.json_normalize(json_data, record_path =['dots'])\n",
        "\n",
        "def to_1D(series):\n",
        "    return pd.Series([x for _list in series for x in _list])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQuIbI08WsIb"
      },
      "outputs": [],
      "source": [
        "ROWS = 75\n",
        "COLS = 100\n",
        "\n",
        "def get_depth(i=0, lst=None):\n",
        "    try:\n",
        "        len(lst[0])\n",
        "        i = get_depth(i+1, lst[0])\n",
        "    except:\n",
        "        pass\n",
        "    return i\n",
        "\n",
        "def get_longest_dot_list(dots):\n",
        "    if get_depth == 1:\n",
        "        return []\n",
        "    try:\n",
        "        isinstance(dots[0][0], list)\n",
        "        idx = 0\n",
        "        if len(dots) > 1:\n",
        "            for i, x in enumerate(dots):\n",
        "                if type(x[0]) != type(0):\n",
        "                    if len(x) >= len(dots[idx]):\n",
        "                        idx = i\n",
        "        return dots[idx]\n",
        "    except IndexError:\n",
        "        # index error indicates that next item is an int of a coord, not a coord itself. so ret first coord\n",
        "        return dots[0]\n",
        "\n",
        "# One hot encoding of user clicks\n",
        "def make_sparse_mat(img_shape, dots):\n",
        "    mat = np.zeros((img_shape[1], img_shape[0]))\n",
        "\n",
        "    # Check for empty dots list case TODO\n",
        "    if len(dots) == 0:\n",
        "        return mat\n",
        "\n",
        "    # Check for 1-D case TODO\n",
        "    if type(dots[0]) == type(0):\n",
        "        mat[dots[1], dots[0]] = 1\n",
        "    else:\n",
        "        for dot in dots:\n",
        "            if not (dot[0] > 800 or dot[1] > 600 or dot[0] < 0 or dot[1] < 0):\n",
        "                mat[dot[1], dot[0]] = 1\n",
        "\n",
        "    return mat\n",
        "\n",
        "def downsample_dots(dots, img_shape):\n",
        "    \"\"\"convert xy coords of annotations down to 600x800 img space\"\"\"\n",
        "    ds_dots = []\n",
        "    x_scaler = COLS / float(img_shape[0])\n",
        "    y_scaler = ROWS / float(img_shape[1])\n",
        "\n",
        "    # Check for empty dots list case\n",
        "    if len(dots) == 0:\n",
        "        return []\n",
        "\n",
        "    # Check for 1-D case TODO\n",
        "    if type(dots[0]) == type(0):\n",
        "        if dots[0] >= img_shape[0] or dots[1] >= img_shape[1] or dots[0] < 0 or dots[1] < 0:\n",
        "            print('Dropped 1 dot - (100%)') # TODO\n",
        "            return []\n",
        "        return [math.floor(dots[0] * x_scaler), math.floor(dots[1] * y_scaler)]\n",
        "\n",
        "    for dot in dots:\n",
        "        try:\n",
        "            if dot[0] >= img_shape[0] or dot[1] >= img_shape[1] or dot[0] < 0 or dot[1] < 0: # if == could we just subtract one? i.e. [2048, 1536] - > [2047, 1535]\n",
        "                # print('DOT OUT OF RANGE:', dot)\n",
        "                continue\n",
        "            ds_dots.append([math.floor(dot[0] * x_scaler), math.floor(dot[1] * y_scaler)]) # TODO\n",
        "        except:\n",
        "            print('ERROR -', dot) # TODO\n",
        "\n",
        "    if len(ds_dots) != len(dots):    \n",
        "        print('Dropped', str(len(dots)-len(ds_dots)), 'dots out of', str(len(dots)), '-', '(' + str((len(dots)-len(ds_dots))*100/len(dots)) + '%)')\n",
        "    return ds_dots\n",
        "\n",
        "def gaussian_filter_density(gt):\n",
        "    #Generates a density map using Gaussian filter transformation\n",
        "    \n",
        "    density = np.zeros(gt.shape, dtype=np.float32)\n",
        "    \n",
        "    gt_count = np.count_nonzero(gt)\n",
        "    \n",
        "    if gt_count == 0:\n",
        "        return density\n",
        "\n",
        "    # FInd out the K nearest neighbours using a KDTree\n",
        "    \n",
        "    pts = np.array(list(zip(np.nonzero(gt)[1].ravel(), np.nonzero(gt)[0].ravel())))\n",
        "    leafsize = 2048\n",
        "    \n",
        "    # build kdtree\n",
        "    tree = scipy.spatial.KDTree(pts.copy(), leafsize=leafsize)\n",
        "    \n",
        "    # query kdtree\n",
        "    distances, locations = tree.query(pts, k=4)\n",
        "\n",
        "        \n",
        "    for i, pt in enumerate(pts):\n",
        "        pt2d = np.zeros(gt.shape, dtype=np.float32)\n",
        "        pt2d[pt[1],pt[0]] = 1.\n",
        "        if gt_count > 3:\n",
        "            sigma = (distances[i][1]+distances[i][2]+distances[i][3])*0.1\n",
        "        else:\n",
        "            sigma = np.average(np.array(gt.shape))/2./2. #case: 1 point\n",
        "        \n",
        "        #Convolve with the gaussian filter\n",
        "        \n",
        "        try:\n",
        "            density += scipy.ndimage.filters.gaussian_filter(pt2d, sigma, mode='constant')\n",
        "        except OverflowError:\n",
        "            print(sigma)\n",
        "            print(distances)\n",
        "            print(gt_count)\n",
        "            raise Exception(\"theres no way its actually this stupid\")\n",
        "        # input_ = np.fft.fft2(pt2d)\n",
        "        # result = scipy.ndimage.fourier_gaussian(input_, sigma)\n",
        "        # density = np.add(density, np.fft.ifft2(result).real, casting=\"unsafe\")\n",
        "    \n",
        "    return density"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x40SzGTVoSJK"
      },
      "outputs": [],
      "source": [
        "original_img_dim = {'BAILa': (2048, 1536), 'DAMOa': (2048, 1536), 'HALFb': (2048, 1536), 'HALFc': (1920, 1080), 'LOCKb': (1920, 1080), 'MAIVb': (2048, 1536), 'MAIVc': (2048, 1536), 'NEKOa': (1920, 1080), 'NEKOb': (2048, 1536), 'NEKOc': (2048, 1536), 'PETEc': (2048, 1536), 'PETEd': (2048, 1536), 'PETEf': (2048, 1536), 'SPIGa': (1920, 1080), 'GEORa': (2048, 1536), 'PETEe': (2048, 1536)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZVYBHuyhTNt"
      },
      "outputs": [],
      "source": [
        "# Get Annotations\n",
        "get_new_batch('CompleteAnnotations_2016-07-11')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crHDXBojrBBP"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/*_gt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJCyvwIQuP8Z",
        "outputId": "dc3ccc71-77c6-405f-c475-a1cfe924423f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing BAILa \n",
            "----------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/internals/blocks.py:937: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  arr_value = np.asarray(value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped 1 dots out of 31 - (3.225806451612903%)\n",
            "Dropped 1 dots out of 121 - (0.8264462809917356%)\n",
            "Dropped 1 dots out of 168 - (0.5952380952380952%)\n",
            "Dropped 1 dots out of 169 - (0.591715976331361%)\n",
            "Dropped 1 dots out of 100 - (1.0%)\n",
            "Dropped 1 dots out of 120 - (0.8333333333333334%)\n",
            "Dropped 1 dots out of 39 - (2.5641025641025643%)\n",
            "Dropped 1 dots out of 32 - (3.125%)\n",
            "Dropped 1 dots out of 29 - (3.4482758620689653%)\n",
            "Dropped 2 dots out of 29 - (6.896551724137931%)\n",
            "Dropped 1 dots out of 117 - (0.8547008547008547%)\n",
            "Dropped 1 dots out of 42 - (2.380952380952381%)\n",
            "Dropped 1 dots out of 36 - (2.7777777777777777%)\n",
            "Dropped 1 dots out of 73 - (1.36986301369863%)\n",
            "Dropped 2 dots out of 14 - (14.285714285714286%)\n",
            "Dropped 1 dots out of 5 - (20.0%)\n",
            "Dropped 2 dots out of 2 - (100.0%)\n",
            "Dropped 6 dots out of 132 - (4.545454545454546%)\n",
            "Dropped 4 dots out of 24 - (16.666666666666668%)\n",
            "Dropped 1 dots out of 6 - (16.666666666666668%)\n",
            "Dropped 1 dots out of 22 - (4.545454545454546%)\n",
            "Dropped 1 dots out of 31 - (3.225806451612903%)\n",
            "Dropped 1 dots out of 144 - (0.6944444444444444%)\n",
            "Dropped 2 dots out of 163 - (1.2269938650306749%)\n",
            "Dropped 2 dots out of 87 - (2.2988505747126435%)\n",
            "Dropped 1 dots out of 173 - (0.5780346820809249%)\n",
            "Dropped 2 dots out of 205 - (0.975609756097561%)\n",
            "Dropped 2 dots out of 141 - (1.4184397163120568%)\n",
            "Dropped 2 dots out of 181 - (1.1049723756906078%)\n",
            "Dropped 1 dots out of 165 - (0.6060606060606061%)\n",
            "Dropped 1 dots out of 162 - (0.6172839506172839%)\n",
            "Dropped 1 dots out of 116 - (0.8620689655172413%)\n",
            "Dropped 1 dots out of 80 - (1.25%)\n",
            "Dropped 1 dots out of 122 - (0.819672131147541%)\n",
            "Dropped 3 dots out of 90 - (3.3333333333333335%)\n",
            "Dropped 1 dots out of 96 - (1.0416666666666667%)\n",
            "Dropped 1 dots out of 63 - (1.5873015873015872%)\n",
            "Dropped 1 dots out of 60 - (1.6666666666666667%)\n",
            "Dropped 1 dots out of 106 - (0.9433962264150944%)\n",
            "Dropped 1 dots out of 131 - (0.7633587786259542%)\n",
            "Dropped 1 dots out of 144 - (0.6944444444444444%)\n",
            "Dropped 1 dots out of 202 - (0.49504950495049505%)\n",
            "Saved BAILa_gt.tgz\n",
            "Removed BAILa and BAILa_gt folders.\n",
            "BAILa processing compelete.\n"
          ]
        }
      ],
      "source": [
        "# slice working list to split workload between us\n",
        "working_list = MASTER_LIST[:1] # TODO\n",
        "\n",
        "# MASTER LOOP\n",
        "# os.chdir(VM_ROOT) TODO\n",
        "for data_split_name in working_list:\n",
        "    \n",
        "    if not os.path.exists(os.path.join('/content/drive/MyDrive/Colab Notebooks/539 Project/data', data_split_name + '_gt.tgz')):\n",
        "        print('\\nProcessing', data_split_name,'\\n----------------------')  \n",
        "    \n",
        "        # Create split dir of ground truths\n",
        "        split_dir = os.path.join(VM_ROOT, data_split_name + '_gt')\n",
        "        os.mkdir(split_dir)\n",
        "        os.chdir(split_dir)\n",
        "\n",
        "        # Get annotations as DataFrame\n",
        "        df = run(target=data_split_name)\n",
        "\n",
        "        # Create heatmap for each image\n",
        "        for i, row in df.iterrows():\n",
        "            # Get image for size to pass to downsample\n",
        "            # img = Image.open(os.path.join('/content', data_split_name, row['imName']) + '.JPG')\n",
        "            # TODO: actually reshape image?\n",
        "            # downsample dots to new img size of 600,800\n",
        "            img_dots = row['xy']\n",
        "            dots = get_longest_dot_list(img_dots)\n",
        "            # print(f\"imName: {row['imName']}\")\n",
        "            # print(f\"img dots: {img_dots}\")\n",
        "            # print(f\"dots: {dots}\")\n",
        "            ds_dots = downsample_dots(dots, original_img_dim[data_split_name])\n",
        "\n",
        "            # make gt heatmap\n",
        "            k_ds = make_sparse_mat((COLS, ROWS), ds_dots)\n",
        "            k_ds = gaussian_filter_density(k_ds)\n",
        "\n",
        "            # save file as an h5 type\n",
        "            with h5py.File(row['imName'] + '_gt.h5', 'w') as f:\n",
        "                f['density'] = k_ds\n",
        "\n",
        "        # Save ground truth zip\n",
        "        save_batch_to_drive(target=data_split_name + '_gt')\n",
        "        print('Saved', data_split_name+'_gt.tgz')\n",
        "        !rm -rf /content/{data_split_name}\n",
        "        !rm -rf /content/{data_split_name}_gt\n",
        "        print('Removed', data_split_name, 'and', data_split_name+'_gt folders.')\n",
        "    print(data_split_name, 'processing compelete.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_dots"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joweGtVKmVCK",
        "outputId": "fc587f26-fa69-48b4-cca3-c3ff2baa9466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[13, 5], [8, 5], [1, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNIWE1Eq4tAm"
      },
      "outputs": [],
      "source": [
        "# # open each image\n",
        "# img = Image.open(os.path.join('/content', data_split_name, row['imName']) + '.JPG')\n",
        "# img.size\n",
        "# img = img.resize((800, 600), Image.ANTIALIAS)\n",
        "#         # image_list.append(img) TODO"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PenguinsPreprocess.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}