{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexjochs/ECE_539_Penguins/blob/model/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "id": "Z0SK51RwOxVQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.preprocessing.image import load_img,img_to_array\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.models import Model,Sequential\n",
        "from keras.layers import *\n",
        "from keras import backend as K\n",
        "from keras.models import model_from_json\n",
        "from matplotlib import cm as CM\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import scipy.io as io\n",
        "from PIL import Image\n",
        "import PIL\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import random\n",
        "import math\n",
        "import sys\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-N73h7SOyP4",
        "outputId": "d795ad5b-d002-40fa-edf7-a05c1c496aec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgTSZe4EOxVS",
        "outputId": "24d32e03-c476-423b-ac76-b27f27a811e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/drive/MyDrive/ANN/Project/data\n"
          ]
        }
      ],
      "source": [
        "K.clear_session()\n",
        "root = r'/content'\n",
        "gdrive_data_filepath = os.path.join(root, 'drive', 'MyDrive', 'ANN', 'Project', 'data')\n",
        "print(root)\n",
        "print(gdrive_data_filepath)\n",
        "os.chdir(root)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_new_batch(target=None):\n",
        "    assert target is not None, f\"can't get specific folder: {target} and load all data\"\n",
        "    tgz_name = target + '.tgz'\n",
        "    folder_path = os.path.join(gdrive_data_filepath, tgz_name)\n",
        "    !tar -xzf {folder_path} -C {root}"
      ],
      "metadata": {
        "id": "kFMeTT-MZjuF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MASTER_LIST = ['BAILa', 'DAMOa', 'HALFb', 'HALFc', 'LOCKb', 'MAIVb', 'MAIVc', 'NEKOa', 'NEKOb', 'NEKOc', 'PETEc', 'PETEd', 'PETEe', 'PETEf', 'SPIGa', 'GEORa']\n",
        "test = [MASTER_LIST, [x + '_gt' for x in MASTER_LIST]]\n",
        "# slice working list to split workload between us\n",
        "working_list = MASTER_LIST[:] # TODO"
      ],
      "metadata": {
        "id": "SJemuNqqdE7V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# for x in working_list:\n",
        "#     !rm -rf {x}\n",
        "#     !rm -rf {x}_gt\n",
        "# !rm -rf /content/content\n",
        "!rm -rf /content/sample_data/"
      ],
      "metadata": {
        "id": "1k1i4MZ-gvT0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all images\n",
        "for (data_split_name, data_split_name_gt) in zip(test[0][:1], test[1][:1]):\n",
        "    if not os.path.exists(data_split_name):\n",
        "        get_new_batch(data_split_name)\n",
        "        get_new_batch(data_split_name_gt)"
      ],
      "metadata": {
        "id": "mBScMzNtZUPf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2OzwF0N1OxVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2b6ac9-cf5b-4b4a-d129-4a43e8ee6832"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/BAILa']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "path_sets = [root + '/' + x for x in test[0][:1]]\n",
        "path_sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_vktQoaOxVU",
        "outputId": "803949b4-28e0-481a-a092-33108a522197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images :  5213\n"
          ]
        }
      ],
      "source": [
        "img_paths = []\n",
        "\n",
        "for path in path_sets:\n",
        "    \n",
        "    for img_path in glob.glob(os.path.join(path, '*.JPG')):\n",
        "        \n",
        "        img_paths.append(str(img_path))\n",
        "        \n",
        "print(\"Total images : \", len(img_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SX5C5GKPOxVV"
      },
      "outputs": [],
      "source": [
        "def create_img(path):\n",
        "    #Function to load,normalize and return image \n",
        "    im = Image.open(path).convert('RGB')\n",
        "    im = im.resize((800, 600), Image.ANTIALIAS)\n",
        "\n",
        "    im = np.array(im)\n",
        "    \n",
        "    im = im/255.0\n",
        "    \n",
        "    im[:,:,0]=(im[:,:,0]-0.485)/0.229\n",
        "    im[:,:,1]=(im[:,:,1]-0.456)/0.224\n",
        "    im[:,:,2]=(im[:,:,2]-0.406)/0.225\n",
        "\n",
        "    # print(im.shape)\n",
        "    #im = np.expand_dims(im,axis  = 0)\n",
        "    return im\n",
        "\n",
        "def get_input(path):\n",
        "    path = path[0] \n",
        "    img = create_img(path)\n",
        "    return(img)\n",
        "    \n",
        "    \n",
        "def get_output(path):\n",
        "    #import target\n",
        "    #resize target\n",
        "    \n",
        "    gt_file = h5py.File(path,'r')\n",
        "    \n",
        "    target = np.asarray(gt_file['density'])\n",
        "    \n",
        "    # img = cv2.resize(target,(int(target.shape[1]/8),int(target.shape[0]/8)),interpolation = cv2.INTER_CUBIC)*64\n",
        "    \n",
        "    # img = np.expand_dims(img, axis=img.ndim)\n",
        "    \n",
        "    # print(img.shape)\n",
        "    \n",
        "    return target\n",
        "    \n",
        "    \n",
        "    \n",
        "def preprocess_input(image,target):\n",
        "    #crop image\n",
        "    #crop target\n",
        "    #resize target\n",
        "    crop_size = (int(image.shape[0]/2),int(image.shape[1]/2))\n",
        "    \n",
        "    \n",
        "    if random.randint(0,9)<= -1:            \n",
        "            dx = int(random.randint(0,1)*image.shape[0]*1./2)\n",
        "            dy = int(random.randint(0,1)*image.shape[1]*1./2)\n",
        "    else:\n",
        "            dx = int(random.random()*image.shape[0]*1./2)\n",
        "            dy = int(random.random()*image.shape[1]*1./2)\n",
        "\n",
        "    #print(crop_size , dx , dy)\n",
        "    img = image[dx : crop_size[0]+dx , dy:crop_size[1]+dy]\n",
        "    \n",
        "    target_aug = target[dx:crop_size[0]+dx,dy:crop_size[1]+dy]\n",
        "    #print(img.shape)\n",
        "\n",
        "    return(img,target_aug)\n",
        "    \n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VO3yxlu4OxVW"
      },
      "outputs": [],
      "source": [
        "#Image data generator \n",
        "def image_generator(files, batch_size = 64):\n",
        "    \n",
        "    while True:\n",
        "        \n",
        "        input_path = np.random.choice(a = files, size = batch_size)\n",
        "        \n",
        "        batch_input = []\n",
        "        batch_output = [] \n",
        "          \n",
        "        #for input_path in batch_paths:\n",
        "        \n",
        "        inputt = get_input(input_path )\n",
        "        out_path = '/content' + input_path[0][:14] + '_gt' + input_path[0][14:].replace('.JPG','_gt.h5')\n",
        "        if not os.path.exists(out_path):\n",
        "            continue\n",
        "        output = get_output(out_path)\n",
        "            \n",
        "       \n",
        "        batch_input += [inputt]\n",
        "        batch_output += [output]\n",
        "    \n",
        "\n",
        "        batch_x = np.array( batch_input )\n",
        "        batch_y = np.array( batch_output )\n",
        "        \n",
        "        yield( batch_x, batch_y )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y0995fZYOxVW"
      },
      "outputs": [],
      "source": [
        "def save_mod(model , str1 , str2):\n",
        "    model.save_weights(str1)\n",
        "    \n",
        "    model_json = model.to_json()\n",
        "    \n",
        "    with open(str2, \"w\") as json_file:\n",
        "        json_file.write(model_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "o8XW1eTQOxVX"
      },
      "outputs": [],
      "source": [
        "def init_weights_vgg(model):\n",
        "    vgg =  VGG16(weights='imagenet', include_top=False)\n",
        "    \n",
        "    # json_file = open('models/VGG_16.json', 'r')\n",
        "    # loaded_model_json = json_file.read()\n",
        "    # json_file.close()\n",
        "    # loaded_model = model_from_json(loaded_model_json)\n",
        "    # loaded_model.load_weights(\"weights/VGG_16.h5\")\n",
        "    \n",
        "    # vgg = loaded_model\n",
        "    \n",
        "    vgg_weights=[]                         \n",
        "    for layer in vgg.layers:\n",
        "        if('conv' in layer.name):\n",
        "            vgg_weights.append(layer.get_weights())\n",
        "    \n",
        "    \n",
        "    offset=0\n",
        "    i=0\n",
        "    while(i<10):\n",
        "        if('conv' in model.layers[i+offset].name):\n",
        "            model.layers[i+offset].set_weights(vgg_weights[i])\n",
        "            i=i+1\n",
        "            #print('h')\n",
        "            \n",
        "        else:\n",
        "            offset=offset+1\n",
        "\n",
        "    return (model)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ivwi0oqeOxVX"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance_loss(y_true, y_pred):\n",
        "    print(y_true.shape, y_pred.shape)\n",
        "    # Euclidean distance as a measure of loss (Loss function) \n",
        "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4RgHCO9DOxVY"
      },
      "outputs": [],
      "source": [
        "# Neural network model : VGG + Conv\n",
        "def CrowdNet():  \n",
        "            #Variable Input Size\n",
        "            rows = 600\n",
        "            cols = 800\n",
        "            \n",
        "            #Batch Normalisation option\n",
        "            \n",
        "            batch_norm = 0\n",
        "            kernel = (3, 3)\n",
        "            init = RandomNormal(stddev=0.01)\n",
        "            model = Sequential() \n",
        "            \n",
        "            #custom VGG:\n",
        "            \n",
        "            if(batch_norm):\n",
        "                model.add(Conv2D(64, kernel_size = kernel, input_shape = (rows,cols,3),activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(MaxPooling2D(strides=2))\n",
        "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(MaxPooling2D(strides=2))\n",
        "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(MaxPooling2D(strides=2))            \n",
        "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same'))\n",
        "                model.add(BatchNormalization())\n",
        "                \n",
        "            else:\n",
        "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same',input_shape = (rows, cols, 3), kernel_initializer = init))\n",
        "                model.add(Conv2D(64, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(MaxPooling2D(strides=2))\n",
        "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(Conv2D(128,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(MaxPooling2D(strides=2))\n",
        "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(Conv2D(256,kernel_size = kernel, activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(MaxPooling2D(strides=2))            \n",
        "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                model.add(Conv2D(512, kernel_size = kernel,activation = 'relu', padding='same', kernel_initializer = init))\n",
        "                \n",
        "                \n",
        "\n",
        "                \n",
        "            #Conv2D\n",
        "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
        "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
        "            model.add(Conv2D(512, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
        "            model.add(Conv2D(256, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
        "            model.add(Conv2D(128, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
        "            model.add(Conv2D(64, (3, 3), activation='relu', dilation_rate = 2, kernel_initializer = init, padding = 'same'))\n",
        "            model.add(Conv2D(1, (1, 1), activation='relu', dilation_rate = 1, kernel_initializer = init, padding = 'same'))\n",
        "        \n",
        "            sgd = SGD(learning_rate = 1e-7, decay = (5*1e-4), momentum = 0.95)\n",
        "            model.compile(optimizer=sgd, loss=euclidean_distance_loss, metrics=['mse'])\n",
        "            \n",
        "            model = init_weights_vgg(model)\n",
        "            \n",
        "            return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eKYkdZdWOxVZ"
      },
      "outputs": [],
      "source": [
        "model = CrowdNet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CmeXV9jbOxVZ",
        "outputId": "1338a0e5-ca3b-4105-fb5d-6753ef0bbc84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_34 (Conv2D)          (None, 600, 800, 64)      1792      \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 600, 800, 64)      36928     \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 300, 400, 64)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_36 (Conv2D)          (None, 300, 400, 128)     73856     \n",
            "                                                                 \n",
            " conv2d_37 (Conv2D)          (None, 300, 400, 128)     147584    \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 150, 200, 128)    0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_38 (Conv2D)          (None, 150, 200, 256)     295168    \n",
            "                                                                 \n",
            " conv2d_39 (Conv2D)          (None, 150, 200, 256)     590080    \n",
            "                                                                 \n",
            " conv2d_40 (Conv2D)          (None, 150, 200, 256)     590080    \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPooling  (None, 75, 100, 256)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_41 (Conv2D)          (None, 75, 100, 512)      1180160   \n",
            "                                                                 \n",
            " conv2d_42 (Conv2D)          (None, 75, 100, 512)      2359808   \n",
            "                                                                 \n",
            " conv2d_43 (Conv2D)          (None, 75, 100, 512)      2359808   \n",
            "                                                                 \n",
            " conv2d_44 (Conv2D)          (None, 75, 100, 512)      2359808   \n",
            "                                                                 \n",
            " conv2d_45 (Conv2D)          (None, 75, 100, 512)      2359808   \n",
            "                                                                 \n",
            " conv2d_46 (Conv2D)          (None, 75, 100, 512)      2359808   \n",
            "                                                                 \n",
            " conv2d_47 (Conv2D)          (None, 75, 100, 256)      1179904   \n",
            "                                                                 \n",
            " conv2d_48 (Conv2D)          (None, 75, 100, 128)      295040    \n",
            "                                                                 \n",
            " conv2d_49 (Conv2D)          (None, 75, 100, 64)       73792     \n",
            "                                                                 \n",
            " conv2d_50 (Conv2D)          (None, 75, 100, 1)        65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16,263,489\n",
            "Trainable params: 16,263,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Q-_TgSohOxVZ"
      },
      "outputs": [],
      "source": [
        "train_gen = image_generator(img_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "scrolled": true,
        "id": "bGq2fKUjOxVa",
        "outputId": "6480a0cd-c3d6-4879-dc7b-99ae17ad3ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "(None, None, None) (None, 75, 100)\n",
            "(None, None, None) (None, 75, 100)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2e270a64374e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m700\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/euclidean_distance_loss/sub/BroadcastGradientArgs' defined at (most recent call last):\n    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n      handler_func(fileobj, events)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 452, in _handle_events\n      self._handle_recv()\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 481, in _handle_recv\n      self._run_callback(callback, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 431, in _run_callback\n      callback(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n      return self.dispatch_shell(stream, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n      handler(stream, idents, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n      user_expressions, allow_stdin)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n      if self.run_code(code, result):\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-28-2e270a64374e>\", line 1, in <module>\n      model.fit(train_gen, epochs=15, steps_per_epoch=700 , verbose=1)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 863, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 531, in minimize\n      loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n      grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/euclidean_distance_loss/sub/BroadcastGradientArgs'\nIncompatible shapes: [1,75,100] vs. [1,600,800]\n\t [[{{node gradient_tape/euclidean_distance_loss/sub/BroadcastGradientArgs}}]] [Op:__inference_train_function_4168]"
          ]
        }
      ],
      "source": [
        "model.fit(train_gen, epochs=15, steps_per_epoch=700 , verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73sJLOJaOxVa"
      },
      "outputs": [],
      "source": [
        "os.chdir(gdrive_data_filepath)\n",
        "os.mkdir('models')\n",
        "os.chdir('models')\n",
        "save_mod(model,\"model_weights.h5\",\"Model.json\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}